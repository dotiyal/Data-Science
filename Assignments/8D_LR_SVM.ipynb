{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86Tvnj5UblTy"
   },
   "source": [
    "## Task-D: Collinear features and their effect on linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qn_eOn2EblT3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMoYWIayblUB"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('task_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfStXG4tblUI",
    "outputId": "ddf4eec6-7f53-4d28-914f-23133957d6d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x*x</th>\n",
       "      <th>2*y</th>\n",
       "      <th>2*z+3*x*x</th>\n",
       "      <th>w</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.581066</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.604025</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>-0.665927</td>\n",
       "      <td>-0.536277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.894309</td>\n",
       "      <td>-0.207835</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.883052</td>\n",
       "      <td>-0.207835</td>\n",
       "      <td>-0.917054</td>\n",
       "      <td>-0.522364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.207552</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>-1.082312</td>\n",
       "      <td>-1.150918</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>-1.166507</td>\n",
       "      <td>0.205738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.364174</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>-0.943643</td>\n",
       "      <td>-1.280666</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>-1.266540</td>\n",
       "      <td>-0.665720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.737687</td>\n",
       "      <td>1.051772</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.744934</td>\n",
       "      <td>1.051772</td>\n",
       "      <td>-0.792746</td>\n",
       "      <td>-0.735054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y         z       x*x       2*y  2*z+3*x*x         w  \\\n",
       "0 -0.581066  0.841837 -1.012978 -0.604025  0.841837  -0.665927 -0.536277   \n",
       "1 -0.894309 -0.207835 -1.012978 -0.883052 -0.207835  -0.917054 -0.522364   \n",
       "2 -1.207552  0.212034 -1.082312 -1.150918  0.212034  -1.166507  0.205738   \n",
       "3 -1.364174  0.002099 -0.943643 -1.280666  0.002099  -1.266540 -0.665720   \n",
       "4 -0.737687  1.051772 -1.012978 -0.744934  1.051772  -0.792746 -0.735054   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIIuomCkblUP"
   },
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1).values\n",
    "Y = data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ydm98u3EblUU"
   },
   "source": [
    "### Doing perturbation test to check the presence of collinearity  \n",
    "\n",
    "#### Task: 1 Logistic Regression\n",
    "<pre>\n",
    "1. <b>Finding the Correlation between the features</b>\n",
    "    a. check the correlation between the features\n",
    "    b. plot heat map of correlation matrix using seaborn heatmap\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x*x</th>\n",
       "      <th>2*y</th>\n",
       "      <th>2*z+3*x*x</th>\n",
       "      <th>w</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.205926</td>\n",
       "      <td>0.812458</td>\n",
       "      <td>0.997947</td>\n",
       "      <td>-0.205926</td>\n",
       "      <td>0.996252</td>\n",
       "      <td>0.583277</td>\n",
       "      <td>0.728290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.205926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>-0.690684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.812458</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807137</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>0.847163</td>\n",
       "      <td>0.674486</td>\n",
       "      <td>0.969990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x*x</th>\n",
       "      <td>0.997947</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>0.807137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>0.997457</td>\n",
       "      <td>0.583803</td>\n",
       "      <td>0.719570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2*y</th>\n",
       "      <td>-0.205926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>-0.690684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2*z+3*x*x</th>\n",
       "      <td>0.996252</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>0.847163</td>\n",
       "      <td>0.997457</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606860</td>\n",
       "      <td>0.764729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.583277</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>0.674486</td>\n",
       "      <td>0.583803</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>0.606860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.728290</td>\n",
       "      <td>-0.690684</td>\n",
       "      <td>0.969990</td>\n",
       "      <td>0.719570</td>\n",
       "      <td>-0.690684</td>\n",
       "      <td>0.764729</td>\n",
       "      <td>0.641750</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x         y         z       x*x       2*y  2*z+3*x*x  \\\n",
       "x          1.000000 -0.205926  0.812458  0.997947 -0.205926   0.996252   \n",
       "y         -0.205926  1.000000 -0.602663 -0.209289  1.000000  -0.261123   \n",
       "z          0.812458 -0.602663  1.000000  0.807137 -0.602663   0.847163   \n",
       "x*x        0.997947 -0.209289  0.807137  1.000000 -0.209289   0.997457   \n",
       "2*y       -0.205926  1.000000 -0.602663 -0.209289  1.000000  -0.261123   \n",
       "2*z+3*x*x  0.996252 -0.261123  0.847163  0.997457 -0.261123   1.000000   \n",
       "w          0.583277 -0.401790  0.674486  0.583803 -0.401790   0.606860   \n",
       "target     0.728290 -0.690684  0.969990  0.719570 -0.690684   0.764729   \n",
       "\n",
       "                  w    target  \n",
       "x          0.583277  0.728290  \n",
       "y         -0.401790 -0.690684  \n",
       "z          0.674486  0.969990  \n",
       "x*x        0.583803  0.719570  \n",
       "2*y       -0.401790 -0.690684  \n",
       "2*z+3*x*x  0.606860  0.764729  \n",
       "w          1.000000  0.641750  \n",
       "target     0.641750  1.000000  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. Checking the correlation between the features\n",
    "correlation_matrix = data.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# b. plot heat map of correlation matrix using seaborn heatmap\n",
    "sns.heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "2. <b>Finding the best model for the given data</b>\n",
    "    a. Train Logistic regression on data(X,Y) that we have created in the above cell\n",
    "    b. Find the best hyper prameter alpha with hyper parameter tuning using k-fold cross validation (grid search CV or         \n",
    "       random search CV make sure you choose the alpha in log space)\n",
    "    c. Creat a new Logistic regression with the best alpha\n",
    "       (search for how to get the best hyper parameter value), name the best model as 'best_model'\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper parameter (CV score = 1.000):\n",
      "{'logistic__C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
    "logistic_model = LogisticRegression()\n",
    "pipe = Pipeline(steps=[('logistic', logistic_model)])\n",
    "folds = KFold(5)\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'logistic__C': np.logspace(-4, 4, 5)\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=folds)\n",
    "search.fit(X, Y)\n",
    "print('Best Hyper parameter (CV score = %0.3f):' % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new logistic regression model with the best alpha\n",
    "c = search.best_params_['logistic__C']\n",
    "best_model = LogisticRegression(C=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "3. <b>Getting the weights with the original data</b>\n",
    "    a. train the 'best_model' with X, Y\n",
    "    b. Check the accuracy of the model 'best_model_accuracy'\n",
    "    c. Get the weights W using best_model.coef_\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's accuracy: 1.0\n",
      "Best model's weights:  [[ 0.72298832 -0.90354834  1.68256456  0.66730582 -0.90354834  0.80372108\n",
      "   0.5096727 ]]\n"
     ]
    }
   ],
   "source": [
    "best_model.fit(X, Y)\n",
    "\n",
    "best_model_accuracy = best_model.score(X, Y)\n",
    "print(\"Best model's accuracy:\", best_model_accuracy)\n",
    "\n",
    "best_model_weights = best_model.coef_\n",
    "print(\"Best model's weights: \", best_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "4. <b>Modifying original data</b>\n",
    "    a. Add a noise(order of 10^-2) to each element of X \n",
    "        and get the new data set X' (X' = X + e)\n",
    "    b. Train the same 'best_model' with data (X', Y)\n",
    "    c. Check the accuracy of the model 'best_model_accuracy_edited'\n",
    "    d. Get the weights W' using best_model.coef_\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's accuracy edited:  1.0\n",
      "Best model's weights W':  [[ 0.72303608 -0.90477213  1.68379332  0.66662916 -0.90057042  0.80726328\n",
      "   0.50706143]]\n"
     ]
    }
   ],
   "source": [
    "# Adding noise(order of 10^-2) to each element of X\n",
    "X_dash = X + np.random.normal(0, 10**-2, X.shape)\n",
    "\n",
    "# train and fit the same 'best_model' with (X', Y)\n",
    "best_model = LogisticRegression(C=c)\n",
    "best_model.fit(X_dash, Y)\n",
    "\n",
    "# accuracy of the model\n",
    "best_model_accuracy_edited = best_model.score(X, Y)\n",
    "print(\"Best model's accuracy edited: \", best_model_accuracy_edited)\n",
    "\n",
    "# the weights W'\n",
    "best_model_weights_w_dash = best_model.coef_\n",
    "print(\"Best model's weights W': \", best_model_weights_w_dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "5. <b> Checking deviations in metric and weights </b>\n",
    "    a. find the difference between 'best_model_accuracy_edited' and 'best_model_accuracy'\n",
    "    b. find the absolute change between each value of W and W' ==> |(W-W')|\n",
    "    c. print the top 4 features which have higher % change in weights compare to the other feature\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between 'best_model_accuracy_edited' and 'best_model_accuracy':  0.0\n",
      "\n",
      "The absolute weights difference:  [[4.77592047e-05 1.22378473e-03 1.22876437e-03 6.76664087e-04\n",
      "  2.97792282e-03 3.54219226e-03 2.61126323e-03]]\n",
      "\n",
      "The Top 4 features having highest percentage weight changes are:  [['2*z+3*x*x' '2*y' 'w' 'z']]\n"
     ]
    }
   ],
   "source": [
    "# the difference between 'best_model_accuracy_edited' and 'best_model_accuracy'\n",
    "difference = abs(best_model_accuracy - best_model_accuracy_edited)\n",
    "print(\"The difference between 'best_model_accuracy_edited' and 'best_model_accuracy': \", difference)\n",
    "\n",
    "# the absolute change between each value of W and W' ==> |(W-W')|\n",
    "absolute_weight_difference = abs(best_model_weights - best_model_weights_w_dash)\n",
    "print('\\nThe absolute weights difference: ', absolute_weight_difference)\n",
    "\n",
    "# the top 4 features which have higher % change in weights compare to the other feature\n",
    "absolute_weight_percentage_difference = absolute_weight_difference * 100\n",
    "# print('\\nThe absolute weights percentage difference: ', absolute_weight_percentage_difference)\n",
    "\n",
    "features = data.columns\n",
    "top_4_features = np.argsort(-absolute_weight_percentage_difference)[:,:4]\n",
    "# print('\\nThe Top 4 features: ', top_4_features)\n",
    "print('\\nThe Top 4 features having highest percentage weight changes are: ', features[top_4_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: 2 Linear SVM\n",
    "\n",
    "<pre>\n",
    "1. Do the same steps (2, 3, 4, 5) we have done in the above task 1.\n",
    "</pre>\n",
    "\n",
    "<strong><font color='red'>Do write the observations based on the results you get from the deviations of weights in both Logistic Regression and linear SVM</font></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "2. <b>Finding the best model for the given data</b>\n",
    "    a. Train Linear SVM model on data(X,Y) that we have created in the above cell\n",
    "    b. Find the best hyper prameter alpha with hyper parameter tuning using k-fold cross validation (grid search CV or         \n",
    "       random search CV make sure you choose the alpha in log space)\n",
    "    c. Creat a new SVM model with the best alpha\n",
    "       (search for how to get the best hyper parameter value), name the best model as 'best_model'\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1).values\n",
    "Y = data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=1.000): \n",
      "{'svm__C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "svm_model = LinearSVC(penalty='l2', max_iter=10000, tol=0.1)\n",
    "\n",
    "pipe = Pipeline(steps=[('svm', svm_model)])\n",
    "folds = KFold(5)\n",
    "param_grid = {\n",
    "    'svm__C': np.logspace(-4, 4, 5)\n",
    "}\n",
    "\n",
    "svm_search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=folds)\n",
    "svm_search.fit(X, Y)\n",
    "\n",
    "print('Best parameter (CV score=%0.3f): ' % svm_search.best_score_)\n",
    "print(svm_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new  model with the best alpha\n",
    "c = svm_search.best_params_['svm__C']\n",
    "best_svm_model = LinearSVC(C=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "3. <b>Getting the weights with the original data</b>\n",
    "    a. train the 'best_model' with X, Y\n",
    "    b. Check the accuracy of the model 'best_model_accuracy'\n",
    "    c. Get the weights W using best_model.coef_\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM model's accuracy: 1.0\n",
      "Best SVM model's weights:  [[ 0.01323056 -0.01280974  0.01791372  0.01305589 -0.01280974  0.01391318\n",
      "   0.01167827]]\n"
     ]
    }
   ],
   "source": [
    "best_svm_model.fit(X, Y)\n",
    "\n",
    "best_svm_model_accuracy = best_svm_model.score(X, Y)\n",
    "print(\"Best SVM model's accuracy:\", best_svm_model_accuracy)\n",
    "\n",
    "best_svm_model_weights = best_svm_model.coef_\n",
    "print(\"Best SVM model's weights: \", best_svm_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "4. <b>Modifying original data</b>\n",
    "    a. Add a noise(order of 10^-2) to each element of X \n",
    "        and get the new data set X' (X' = X + e)\n",
    "    b. Train the same 'best_model' with data (X', Y)\n",
    "    c. Check the accuracy of the model 'best_model_accuracy_edited'\n",
    "    d. Get the weights W' using best_model.coef_\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM model's accuracy edited:  1.0\n",
      "Best SVM model's weights W':  [[ 0.01324642 -0.01280803  0.01790376  0.01309328 -0.01279401  0.01392715\n",
      "   0.01168332]]\n"
     ]
    }
   ],
   "source": [
    "# Adding noise(order of 10^-2) to each element of X\n",
    "\n",
    "X_dash = X + np.random.normal(0, 10**-2, X.shape)\n",
    "\n",
    "# train and fit the same 'best_model' with (X', Y)\n",
    "best_svm_model = LinearSVC(C=c)\n",
    "best_svm_model.fit(X_dash, Y)\n",
    "\n",
    "# accuracy of the model\n",
    "best_svm_model_accuracy_edited = best_svm_model.score(X, Y)\n",
    "print(\"Best SVM model's accuracy edited: \", best_svm_model_accuracy_edited)\n",
    "\n",
    "# the weights W'\n",
    "best_svm_model_weights_w_dash = best_svm_model.coef_\n",
    "print(\"Best SVM model's weights W': \", best_svm_model_weights_w_dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "5. <b> Checking deviations in metric and weights </b>\n",
    "    a. find the difference between 'best_model_accuracy_edited' and 'best_model_accuracy'\n",
    "    b. find the absolute change between each value of W and W' ==> |(W-W')|\n",
    "    c. print the top 4 features which have higher % change in weights compare to the other feature\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between 'best_svm_model_accuracy_edited' and 'best_svm_model_accuracy':  0.0\n",
      "\n",
      "The absolute weights difference SVM:  [[1.58618545e-05 1.70315987e-06 9.95589050e-06 3.73872305e-05\n",
      "  1.57287379e-05 1.39704051e-05 5.05281642e-06]]\n",
      "\n",
      "The Top 4 features having highest percentage weight changes are (SVM):  [['x*x' 'x' '2*y' '2*z+3*x*x']]\n"
     ]
    }
   ],
   "source": [
    "# the difference between 'best_model_accuracy_edited' and 'best_model_accuracy'\n",
    "difference_svm = abs(best_svm_model_accuracy - best_svm_model_accuracy_edited)\n",
    "print(\"The difference between 'best_svm_model_accuracy_edited' and 'best_svm_model_accuracy': \", difference_svm)\n",
    "\n",
    "# the absolute change between each value of W and W' ==> |(W-W')|\n",
    "absolute_weight_difference_svm = abs(best_svm_model_weights - best_svm_model_weights_w_dash)\n",
    "print('\\nThe absolute weights difference SVM: ', absolute_weight_difference_svm)\n",
    "\n",
    "# the top 4 features which have higher % change in weights compare to the other feature\n",
    "absolute_weight_percentage_difference_svm = absolute_weight_difference_svm * 100\n",
    "#print('\\nThe absolute weights percentage difference SVM: ', absolute_weight_percentage_difference_svm)\n",
    "\n",
    "features_svm = data.columns\n",
    "top_4_features_svm = np.argsort(-absolute_weight_percentage_difference_svm)[:,:4]\n",
    "#print('\\nThe Top 4 features for SVM: ', top_4_features_svm)\n",
    "print('\\nThe Top 4 features having highest percentage weight changes are (SVM): ', features_svm[top_4_features_svm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:-\n",
    "1. The model accuracy for both Logistic Regression and Linear SVM, are not affected by the Multi colinearity.\n",
    "2. The Top 4 features with maximum absolute percentage weight difference from original data to noisy data we can conclude that features having multi colinearity and high correlation is affected by added noise and coefficient are changing for those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "8D_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
